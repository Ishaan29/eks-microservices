name: CI/CD Pipeline - Dev Branch

on:
  push:
    branches:
      - cicd
      - cicd-dev

  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY: 440491339319.dkr.ecr.us-east-1.amazonaws.com
  KUBERNETES_NAMESPACE: dev

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Set image version
        id: set-version
        run: |
          VERSION="v${GITHUB_SHA::8}"
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "Image version: $VERSION"

      - name: Build and push Docker images
        id: build-images
        run: |
          VERSION="${{ steps.set-version.outputs.version }}"
          ECR_REGISTRY="${{ env.ECR_REGISTRY }}"

          # Define microservices (add more as needed)
          # Format: "service-name:directory"
          MICROSERVICES=("frontend:frontend" "inventory-api:backend/inventory-api" "orders-api:backend/orders-api" "products-api:backend/products-api")

          # Build and push each microservice
          for SERVICE_ENTRY in "${MICROSERVICES[@]}"; do
            SERVICE="${SERVICE_ENTRY%%:*}"
            BUILD_DIR="${SERVICE_ENTRY#*:}"

            echo "Building $SERVICE from $BUILD_DIR..."
            IMAGE_NAME="eks-microservices-${SERVICE}"
            IMAGE_TAG="${ECR_REGISTRY}/${IMAGE_NAME}:${VERSION}"

            # Check if Dockerfile exists
            if [ ! -f "./${BUILD_DIR}/Dockerfile" ]; then
              echo "Warning: Dockerfile not found at ./${BUILD_DIR}/Dockerfile, skipping..."
              continue
            fi

            # Build Docker image
            docker build -t "${IMAGE_TAG}" ./${BUILD_DIR} || {
              echo "Failed to build ${SERVICE}"
              exit 1
            }

            # Push to ECR
            docker push "${IMAGE_TAG}" || {
              echo "Failed to push ${SERVICE}"
              exit 1
            }

            echo "Successfully built and pushed ${IMAGE_TAG}"
            echo "${SERVICE}_image=${IMAGE_TAG}" >> $GITHUB_OUTPUT
          done

      - name: Configure kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Set up kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ secrets.EKS_CLUSTER_NAME }}

      - name: Update Kubernetes deployments
        id: update-deployments
        run: |
          VERSION="${{ steps.set-version.outputs.version }}"
          ECR_REGISTRY="${{ env.ECR_REGISTRY }}"

          # Define microservices (must match build step)
          MICROSERVICES=("frontend" "inventory-api" "orders-api" "products-api")

          # Update deployment files
          for SERVICE in "${MICROSERVICES[@]}"; do
            # Map service name to deployment file name (remove -api suffix for file lookup)
            DEPLOYMENT_FILE_NAME="${SERVICE%-api}"
            DEPLOYMENT_FILE="k8s/deployment-${DEPLOYMENT_FILE_NAME}.yaml"
            IMAGE_NAME="eks-microservices-${SERVICE}"
            NEW_IMAGE="${ECR_REGISTRY}/${IMAGE_NAME}:${VERSION}"

            if [ -f "$DEPLOYMENT_FILE" ]; then
              # Backup current deployment
              cp "$DEPLOYMENT_FILE" "${DEPLOYMENT_FILE}.backup"

              # Update image in deployment file using sed
              # Pattern matches: image: <any-ecr-url>/eks-microservices-<service>:<any-tag>
              # Try to replace just the version tag first
              if sed -i "s|\(image:.*${IMAGE_NAME}:\).*|\1${VERSION}|g" "$DEPLOYMENT_FILE" 2>/dev/null; then
                echo "Updated image version tag"
              else
                # Fallback: replace entire image line
                sed -i "s|image:.*${IMAGE_NAME}.*|image: ${NEW_IMAGE}|g" "$DEPLOYMENT_FILE"
              fi

              echo "Updated ${DEPLOYMENT_FILE} with image: ${NEW_IMAGE}"
            else
              echo "Warning: ${DEPLOYMENT_FILE} not found, skipping..."
            fi
          done

      - name: Apply Kubernetes deployments
        id: apply-deployments
        continue-on-error: true
        run: |
          VERSION="${{ steps.set-version.outputs.version }}"
          ROLLBACK_NEEDED=false

          # Define microservices (must match build step)
          MICROSERVICES=("frontend" "inventory-api" "orders-api" "products-api")

          for SERVICE in "${MICROSERVICES[@]}"; do
            # Map service name to deployment file name (remove -api suffix for file lookup)
            DEPLOYMENT_FILE_NAME="${SERVICE%-api}"
            DEPLOYMENT_FILE="k8s/deployment-${DEPLOYMENT_FILE_NAME}.yaml"
            DEPLOYMENT_NAME="${SERVICE}-deployment"
            
            if [ -f "$DEPLOYMENT_FILE" ]; then
              echo "Applying deployment for ${SERVICE}..."
              
              # Apply deployment
              kubectl apply -f "$DEPLOYMENT_FILE" -n ${{ env.KUBERNETES_NAMESPACE }} || {
                echo "Failed to apply ${DEPLOYMENT_FILE}"
                ROLLBACK_NEEDED=true
                continue
              }
              
              # Wait for rollout to complete
              kubectl rollout status deployment/${DEPLOYMENT_NAME} -n ${{ env.KUBERNETES_NAMESPACE }} --timeout=5m || {
                echo "Rollout failed for ${DEPLOYMENT_NAME}"
                ROLLBACK_NEEDED=true
                continue
              }
              
              echo "Successfully deployed ${SERVICE}"
            else
              echo "Warning: ${DEPLOYMENT_FILE} not found, skipping deployment..."
            fi
          done
          
          if [ "$ROLLBACK_NEEDED" = true ]; then
            echo "rollback_needed=true" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Rollback on failure
        if: failure() && steps.apply-deployments.outcome == 'failure'
        run: |
          echo "Deployment failed, rolling back..."

          # Define microservices (must match build step)
          MICROSERVICES=("frontend" "inventory-api" "orders-api" "products-api")

          for SERVICE in "${MICROSERVICES[@]}"; do
            DEPLOYMENT_NAME="${SERVICE}-deployment"

            echo "Rolling back ${DEPLOYMENT_NAME}..."
            kubectl rollout undo deployment/${DEPLOYMENT_NAME} -n ${{ env.KUBERNETES_NAMESPACE }} || {
              echo "Warning: Failed to rollback ${DEPLOYMENT_NAME}"
            }

            # Restore deployment file from backup
            # Map service name to deployment file name (remove -api suffix for file lookup)
            DEPLOYMENT_FILE_NAME="${SERVICE%-api}"
            DEPLOYMENT_FILE="k8s/deployment-${DEPLOYMENT_FILE_NAME}.yaml"
            if [ -f "${DEPLOYMENT_FILE}.backup" ]; then
              mv "${DEPLOYMENT_FILE}.backup" "$DEPLOYMENT_FILE"
              echo "Restored ${DEPLOYMENT_FILE} from backup"
            fi
          done
          
          echo "Rollback completed"

      - name: Cleanup backup files
        if: always()
        run: |
          find k8s -name "*.backup" -delete || true

  deploy-observability:
    runs-on: ubuntu-latest
    needs: build-and-deploy
    if: success()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Set up kubeconfig for observability
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ secrets.EKS_CLUSTER_NAME }}

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'

      - name: Add Helm repos
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update

      - name: Create monitoring namespace
        run: |
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -

      - name: Create Grafana Dashboard ConfigMaps
        run: |
          kubectl create configmap grafana-dashboard-cpu \
            --from-file=Observability/dashboards/cpu-dashboard.json \
            -n monitoring \
            --dry-run=client -o yaml | kubectl apply -f -

          kubectl create configmap grafana-dashboard-errors \
            --from-file=Observability/dashboards/errors-dashboard.json \
            -n monitoring \
            --dry-run=client -o yaml | kubectl apply -f -

          kubectl create configmap grafana-dashboard-latency \
            --from-file=Observability/dashboards/latency-dashboard.json \
            -n monitoring \
            --dry-run=client -o yaml | kubectl apply -f -

          # Label the ConfigMaps for Grafana to discover them
          kubectl label configmap grafana-dashboard-cpu grafana_dashboard=1 -n monitoring --overwrite
          kubectl label configmap grafana-dashboard-errors grafana_dashboard=1 -n monitoring --overwrite
          kubectl label configmap grafana-dashboard-latency grafana_dashboard=1 -n monitoring --overwrite

      - name: Cleanup existing Helm releases
        run: |
          # Check for existing releases in other namespaces and uninstall them
          if helm list -A | grep -q "monitoring.*prommonitoring"; then
            echo "Found existing monitoring release in prommonitoring namespace, uninstalling..."
            helm uninstall monitoring -n prommonitoring || true
          fi
          
          # Clean up orphaned ClusterRoles if they exist
          kubectl delete clusterrole monitoring-grafana-clusterrole --ignore-not-found=true
          kubectl delete clusterrolebinding monitoring-grafana-clusterrolebinding --ignore-not-found=true
          
          # Clean up any stuck StatefulSets and PVCs from previous deployments
          echo "Cleaning up stuck StatefulSets and PVCs if they exist..."
          kubectl delete statefulset prometheus-prometheus-stack-prometheus -n monitoring --ignore-not-found=true
          kubectl delete statefulset alertmanager-prometheus-stack-alertmanager -n monitoring --ignore-not-found=true
          kubectl delete pvc --all -n monitoring --ignore-not-found=true
          
          # Wait for cleanup to complete
          sleep 5

      - name: Deploy Kube-Prometheus Stack
        run: |
          helm upgrade --install monitoring prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --create-namespace \
            -f Observability/kube-stack/kube-stack-values.yaml \
            --set grafana.adminPassword=${{ secrets.GRAFANA_ADMIN_PASSWORD }} \
            --set clusterName=${{ secrets.EKS_CLUSTER_NAME }} \
            --force \
            --cleanup-on-fail \
            --wait \
            --timeout 10m

      - name: Deploy Fluent Bit
        run: |
          helm upgrade --install fluent-bit eks/aws-for-fluent-bit \
            --namespace kube-system \
            -f Observability/fluent-bit/fluent-bit-values.yaml \
            --set cloudWatch.region=${{ env.AWS_REGION }} \
            --wait \
            --timeout 5m

      - name: Wait for Grafana to be ready
        run: |
          kubectl wait --for=condition=available --timeout=300s deployment/monitoring-grafana -n monitoring

      - name: Patch Grafana and Prometheus services to LoadBalancer
        run: |
          kubectl patch svc monitoring-grafana -n monitoring -p '{"spec": {"type": "LoadBalancer"}}'
          kubectl patch svc monitoring-kube-prometheus-prometheus -n monitoring -p '{"spec": {"type": "LoadBalancer"}}' || true

      - name: Get observability endpoints
        run: |
          echo "Waiting for LoadBalancer endpoints..."
          sleep 30

          echo "=== Grafana Service ==="
          kubectl get svc monitoring-grafana -n monitoring

          echo ""
          echo "=== Prometheus Service ==="
          kubectl get svc monitoring-kube-prometheus-prometheus -n monitoring || echo "Prometheus service not found"

          echo ""
          echo "=== Fluent Bit Status ==="
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-for-fluent-bit

